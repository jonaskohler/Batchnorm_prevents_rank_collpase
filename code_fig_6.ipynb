{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code you need pytorch and torchvision installed on your device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import IPython\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import torch.nn.functional as func\n",
    "import pickle\n",
    "dtype = torch.FloatTensor\n",
    "dtype_labels = torch.LongTensor\n",
    "\n",
    "no_of_hl= 30   #second arg is the number of hidden layers\n",
    "\n",
    "HUs=128 # number of hidden units\n",
    "step_size =0.01 # stepsize\n",
    "min_batch_size = 32 # minibatch for SGD\n",
    "batch_norm_size = 10\n",
    "hidden_layers=np.ones(no_of_hl,dtype=int)*HUs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting Fashion-minist using torchvision\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='Fashion_MNIST_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=min_batch_size,\n",
    "                                          shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making layer sizes\n",
    "# loading a mini-batch out of the dataset\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.view(-1,1*28*28)\n",
    "D_in=images[0].shape[0]\n",
    "D_out=10 # for mnist is 10 \n",
    "_layers=np.append(hidden_layers,[D_out])\n",
    "layers=np.append([D_in], _layers)  #this variable contains the Network arcitecture in terms of units in each layer,e.g. 5,10,10,1 (D_in,hidden 1, hidden 2, D_out)\n",
    "print('Network architecture (no of units in layer): ',layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#networks \n",
    "def normalize_center(A):\n",
    "    d=A.shape[1]\n",
    "    means=torch.mean(A,dim=0)\n",
    "    stds=torch.std(A,dim=0)\n",
    "    stds=stds+0.001\n",
    "    A_scaled=(A-means.reshape(1,d))/stds.reshape(1,d) ## normalized along COLS (feature lives in row for me)\n",
    "    return means.detach(), stds.detach(), A_scaled\n",
    "\n",
    "\n",
    "\n",
    "# MLP without batch normalization \n",
    "class MlpPlane(torch.nn.Module): \n",
    "    def __init__(self,h_sizes): \n",
    "        super(MlpPlane,self).__init__()\n",
    "        self.h_sizes = h_sizes\n",
    "        self.layers = nn.ModuleList()\n",
    "        for k in range(len(h_sizes)-1):\n",
    "            linear_module = nn.Linear(h_sizes[k].item(), h_sizes[k+1].item())\n",
    "            variance = np.sqrt(2.0/(h_sizes[k].item() + h_sizes[k+1].item()))\n",
    "            linear_module.weight.data.normal_(0.0, variance)\n",
    "            self.layers.append(linear_module)\n",
    "    def forward(self,x):\n",
    "        for k in range(len(self.h_sizes)-2): \n",
    "            x = torch.relu(self.layers[k](x))\n",
    "        return self.layers[len(self.h_sizes)-2](x)\n",
    "    def get_weights(self): \n",
    "        ws = [None]*(len(self.h_sizes)-1)\n",
    "        for k in range(len(self.h_sizes)-1): \n",
    "            ws[k] = self.layers[k].weight\n",
    "        return ws\n",
    "    def getlayerloss(self,x,layer_num): # approximate \n",
    "        for k in range(layer_num): \n",
    "            x = torch.relu(self.layers[k](x))\n",
    "       # for k in range(layer_num+1): \n",
    "        #    x = torch.relu(self.layers[k](x))\n",
    "        x=self.layers[layer_num](x)\n",
    "        \n",
    "        M = x.t().mm(x)/x.size(0)\n",
    "        return torch.trace(M.mm(M))/torch.trace(M)**2 #+ troch.norm(M)\n",
    "    def getblanceloss(self,x):\n",
    "        lo = 0 \n",
    "        for k in range(len(self.h_sizes)-1): \n",
    "            x = torch.relu(self.layers[k](x))\n",
    "            M = x.mm(x.t())/float(min_batch_size)\n",
    "#             print(M.size())\n",
    "            lo = lo + torch.trace(M.mm(M))/torch.trace(M)**2 #+ torch.norm(M)\n",
    "        return lo\n",
    "##### BATCH Normalization \n",
    "\n",
    "\n",
    "class BNN(nn.Module): #note that the actual number of hidden laers is no_of_hidden_layers+1\n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10,no_of_hidden_layers=no_of_hl,seed=None, act=torch.tanh):\n",
    "        super(BNN, self).__init__()\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.Win = nn.Linear(input_dim, hidden_dim,bias=True)        \n",
    "        self.layers = torch.nn.ModuleList([nn.Linear(hidden_dim, hidden_dim,bias=True) for _ in range(no_of_hidden_layers)])\n",
    "        self.BNlayers = torch.nn.ModuleList([nn.BatchNorm1d(hidden_dim,momentum=0.0) for _ in range(no_of_hidden_layers)])\n",
    "        self.Wout = nn.Linear(hidden_dim, output_dim,bias=True)\n",
    "        self.act=act\n",
    "    def forward(self, input):\n",
    "        means_list=[]\n",
    "        stds_list=[]\n",
    "        _x = self.Win(input)\n",
    "        x=self.act(_x)\n",
    "        \n",
    "        for layer,BN in zip(self.layers,self.BNlayers):\n",
    "            _x=BN(layer(x))\n",
    "            x=self.act(_x)\n",
    "        y_pred = self.Wout(x)\n",
    "        return y_pred\n",
    "\n",
    "    def getlayerloss(self,x,layer_num): # approximate \n",
    "        \n",
    "        _x = self.Win(x)\n",
    "        x=self.act(_x)\n",
    "        counter=0\n",
    "\n",
    "        for layer,BN in zip(self.layers,self.BNlayers):\n",
    "            if counter<=layer_num:\n",
    "                _x=BN(layer(x))\n",
    "                x=self.act(_x)\n",
    "                counter=counter+1\n",
    "        \n",
    "        M = x.t().mm(x)/x.size(0)\n",
    "        return torch.trace(M.mm(M))/torch.trace(M)**2 #+ troch.norm(M)\n",
    "\n",
    "class MlpBatch(MlpPlane): \n",
    "    def __init__(self,h_sizes): \n",
    "        super(MlpBatch,self).__init__(h_sizes)\n",
    "        self.batches = nn.ModuleList()\n",
    "        for k in range(len(h_sizes)-2): \n",
    "            self.batches.append(torch.nn.BatchNorm1d(num_features=h_sizes[k+1].item(),momentum=0.0))\n",
    "    def forward(self,x):\n",
    "        for k in range(len(self.h_sizes)-2): \n",
    "            x = torch.relu(self.batches[k](self.layers[k](x)))\n",
    "        return self.layers[len(self.h_sizes)-2](x)\n",
    "    \n",
    "    def getlayerloss(self,x,layer_num): # approximate \n",
    "\n",
    "        counter=0\n",
    "        for k in range(layer_num): \n",
    "            x = torch.relu(self.batches[k](self.layers[k](x)))\n",
    "\n",
    "        \n",
    "        M = x.t().mm(x)/x.size(0)\n",
    "        return torch.trace(M.mm(M))/torch.trace(M)**2 #+ troch.norm(M)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as f\n",
    "def run_training(mlp, epochs = 15,ss = step_size): # this function runs SGD training for the given network mlp using stepsize ss for #epoches \n",
    "    errors = []\n",
    "    h_ranks=[]\n",
    "    criterion = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "    opt2= torch.optim.SGD(mlp.parameters(),lr =ss )\n",
    "    loss_epoch = 0 \n",
    "    data_counter = 0 \n",
    "    N = 50000\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs= inputs.view(-1,1*28*28)\n",
    "            inputs = Variable(inputs).type(dtype)\n",
    "            labels = Variable(labels).type(dtype_labels)\n",
    "            outputs = mlp.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_epoch += loss.detach().numpy()*inputs.shape[0]/float(N)\n",
    "            data_counter += inputs.shape[0]\n",
    "    print(loss_epoch)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        avg_rank=(mlp.getlayerloss(inputs,1)+mlp.getlayerloss(inputs,5)+mlp.getlayerloss(inputs,10)+mlp.getlayerloss(inputs,15)\\\n",
    "                +mlp.getlayerloss(inputs,20)+mlp.getlayerloss(inputs,25)+mlp.getlayerloss(inputs,29))/7\n",
    "        h_ranks.append(avg_rank.item())\n",
    "\n",
    "    errors.append(loss_epoch)\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        print('new epoch--------')\n",
    "        loss_epoch = 0 \n",
    "        data_counter = 0 \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            opt2.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs= inputs.view(-1,28*28)\n",
    "    #         inputs = f.normalize(inputs, p=2, dim=1)\n",
    "            inputs = Variable(inputs).type(dtype)\n",
    "            labels = Variable(labels).type(dtype_labels)\n",
    "            outputs = mlp.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt2.step()\n",
    "            loss_epoch += loss.detach().numpy()*inputs.shape[0]/float(N)\n",
    "    #         data_counter += inputs.shape[0]\n",
    "        print('loss:',loss_epoch)\n",
    "        with torch.no_grad():\n",
    "            avg_rank=(mlp.getlayerloss(inputs,1)+mlp.getlayerloss(inputs,5)+mlp.getlayerloss(inputs,10)+mlp.getlayerloss(inputs,15)\\\n",
    "                    +mlp.getlayerloss(inputs,20)+mlp.getlayerloss(inputs,25)+mlp.getlayerloss(inputs,29))/7\n",
    "            h_ranks.append(avg_rank.item())\n",
    "        print('hrank:',1./avg_rank)\n",
    "\n",
    "\n",
    "        errors.append(loss_epoch)\n",
    "    return errors,h_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a mini-batch out of the dataset\n",
    "dataiter = iter(train_loader)\n",
    "xb, labels = dataiter.next()\n",
    "xb = xb.view(-1,1*28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "xb = images\n",
    "soft_ranks = []\n",
    "\n",
    "\n",
    "for i in range(4): \n",
    "    models.append(MlpPlane(layers))\n",
    "\n",
    "loss = models[0].getlayerloss(xb,9)\n",
    "print(loss.data)\n",
    "soft_ranks.append(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bsize = xb.shape[0]\n",
    "mlp = MlpPlane(layers)\n",
    "ss = 0.1\n",
    "outnet = mlp.forward(xb)\n",
    "outnet = outnet.detach().numpy()\n",
    "_,s,_ = np.linalg.svd(outnet)\n",
    "rank_track = []\n",
    "\n",
    "print('===============')\n",
    "print('Before optimization: normalized eigenvalues of input-output mapping (normalized by the trace)')\n",
    "print(s/sum(s))\n",
    "print('===============')\n",
    "ranges = [5,75]  \n",
    "\n",
    "\n",
    "for mm in range(len(models)-1):\n",
    "    print('============================')\n",
    "    print('new model!!')\n",
    "    avg_loss=[]\n",
    "    for kk in range(no_of_hl+1): # we layerwise optimize the established lower-bound on the rank function \n",
    "\n",
    "        if kk>0 and kk%4==0:\n",
    "            for i in range(ranges[mm]):\n",
    "                if i==0:\n",
    "                    loss = mlp.getlayerloss(xb,kk) #compute the approx rank of layer kk\n",
    "                    print(\"layer \"+str(kk)+' --before--')\n",
    "                    print(loss.data)\n",
    "                    print('-------')\n",
    "            \n",
    "                for j in np.arange(0,kk+1):\n",
    "                    mlp.layers[j].weight.data = mlp.layers[j].weight.data- ss*torch.autograd.grad(loss, mlp.layers[j].weight, create_graph=True)[0].data\n",
    "\n",
    "\n",
    "                loss = mlp.getlayerloss(xb,kk)\n",
    "                avg_loss.append(loss.item())\n",
    "\n",
    "            print(\"layer \"+str(kk)+\" --after \" + str(ranges[mm]) + \" iterations--\")\n",
    "            print(loss.data)\n",
    "            print('-------')\n",
    "    models[mm+1].load_state_dict(mlp.state_dict()) ## Copies parameters and buffers from state_dict into this module and its descendants\n",
    "    soft_ranks.append(np.mean(avg_loss))\n",
    "    \n",
    "## save an extra model that will be regularized\n",
    "extra_model=MlpPlane(layers)\n",
    "extra_model.load_state_dict(mlp.state_dict())\n",
    "    \n",
    "outnet = mlp.forward(xb)\n",
    "outnet = outnet.detach().numpy()\n",
    "_,s,_ = np.linalg.svd(outnet)\n",
    "print('===============')\n",
    "print('After optimization: normalized eigenvalues of input-output mapping (normalized by the trace)')\n",
    "print(s/sum(s))\n",
    "print('===============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_train = []\n",
    "xb = images\n",
    "for i in range(len(models)): \n",
    "    mod = MlpPlane(layers)\n",
    "    mod.load_state_dict(models[i].state_dict())\n",
    "    models_train.append(mod)\n",
    "\n",
    "for model in models_train: \n",
    "    loss = model.getlayerloss(xb,no_of_hl)\n",
    "    print(loss.data)\n",
    "    print(model.layers[no_of_hl].weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "rankz = []\n",
    "for i in range(len(models)): \n",
    "  terror,_rank = run_training(models[i],epochs = 15,ss = 0.01)\n",
    "  errors.append(terror)\n",
    "  rankz.append(_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rankz)):\n",
    "    rankz[i]=1./np.array(rankz[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_batch = BNN(no_of_hidden_layers=no_of_hl)\n",
    "\n",
    "errors_batch, rankz_batch = run_training(mlp_batch,epochs = 15,ss = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screw BN up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_initialization(model,hl,input_dim=784, hidden_dim=128, output_dim=10):\n",
    "    for p in model.parameters():\n",
    "        C=torch.FloatTensor(p.data.shape).uniform_(0, 0.1).type(dtype)\n",
    "        p.data=C.data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_batch = BNN(no_of_hidden_layers=no_of_hl,seed=torch.LongTensor(1).random_(0, 100))\n",
    "\n",
    "bad_initialization(mlp_batch,no_of_hl)\n",
    "errors_batch_uni, rankz_batch_uni = run_training(mlp_batch,epochs = 15,ss = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_batch_1=errors_batch\n",
    "errors_batch_uni_1=errors_batch_uni\n",
    "rankz_1=rankz\n",
    "errors_1=errors\n",
    "rankz_batch_1=rankz_batch\n",
    "rankz_batch_uni_1=rankz_batch_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_run1 = {'run_id': np.ones(len(errors_batch),dtype=np.int8)*1,'errors_mlp_1': errors[0],'errors_mlp_2': errors[1],\n",
    "             'errors_mlp_3': errors[2],'errors_mlp_4': errors[3],\n",
    "             'ranks_mlp_1': rankz[0],'ranks_mlp_2': rankz[1],\n",
    "             'ranks_mlp_3': rankz[2],'ranks_mlp_4': rankz[3],\n",
    "             'errors_batch': errors_batch, \n",
    "             'errors_batch_uni': errors_batch_uni,'rankz_batch': rankz_batch,\n",
    "             'rankz_batch_uni':rankz_batch_uni}  \n",
    "    \n",
    "pd_run1 = pd.DataFrame(dict_run1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data=pd.concat([pd_run1,pd_run2,pd_run3])\n",
    "data=pd_run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rankz_batch_uni\"]=1./data[\"rankz_batch_uni\"]\n",
    "data[\"rankz_batch\"]=1./data[\"rankz_batch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "\n",
    "\n",
    "sns.lineplot(data=data,x=data.index,y=\"errors_mlp_1\",label=\"SGD no pre-training\", ci=95)\n",
    "sns.lineplot(data=data,x=data.index,y=\"errors_mlp_2\",label=\"SGD low pre-training\", ci=95)\n",
    "sns.lineplot(data=data,x=data.index,y=\"errors_mlp_3\",label=\"SGD high pre-training\", ci=95)\n",
    "#sns.lineplot(data=data,x=data.index,y=\"errors_mlp_4\",label=\"MLP 4\", marker=\"o\", ci=95)\n",
    "\n",
    "sns.lineplot(data=data,x=data.index,y=\"errors_batch\",label=\"BN $W\\sim U[-a,a]$\", marker=\"X\", ci=95)\n",
    "\n",
    "ax=sns.lineplot(data=data,x=data.index,y=\"errors_batch_uni\",label=\"BN $W\\sim U[0,2a]$\",  marker=\"X\", ci=95)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=14)\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=12.5)\n",
    "ax.yaxis.set_tick_params(labelsize=12.5)\n",
    "plt.ylabel('Training loss',fontsize=14)\n",
    "plt.xlabel('Epochs',fontsize=14)\n",
    "#plt.title(\"10 hidden layers\", fontsize=15)\n",
    "plt.savefig(\"fig_pretrain_a.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "\n",
    "\n",
    "sns.lineplot(data=data,x=data.index,y=\"ranks_mlp_1\",label=\"SGD no pre-training\",  ci=95)\n",
    "sns.lineplot(data=data,x=data.index,y=\"ranks_mlp_2\",label=\"SGD low pre-training\", ci=95)\n",
    "sns.lineplot(data=data,x=data.index,y=\"ranks_mlp_3\",label=\"SGD high pre-training\",  ci=95)\n",
    "#sns.lineplot(data=data,x=data.index,y=\"ranks_mlp_4\",label=\"MLP 4\", marker=\"o\", ci=95)\n",
    "\n",
    "sns.lineplot(data=data,x=data.index,y=\"rankz_batch\",label=\"BN W\\sim$ U[-a,a]$\", marker=\"X\", ci=95)\n",
    "\n",
    "ax=sns.lineplot(data=data,x=data.index,y=\"rankz_batch_uni\",label=\"BN $W\\sim U[0,2a]$\", marker=\"X\", ci=95)\n",
    "\n",
    "\n",
    "ax.xaxis.set_tick_params(labelsize=12.5)\n",
    "ax.yaxis.set_tick_params(labelsize=12.5)\n",
    "plt.ylabel('Lower bound on rank',fontsize=14)\n",
    "plt.xlabel('Epochs',fontsize=14)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5),fontsize=14)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
